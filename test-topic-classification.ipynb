{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "\n",
    "def load_words_from_json(filepath: str) -> List[str]:\n",
    "    \"\"\"Load words from a JSON file where keys are words\n",
    "\n",
    "    Args:\n",
    "        filepath (str): Path to the JSON file.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of words.\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return list(data.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'aa',\n",
       " 'aaa',\n",
       " 'aah',\n",
       " 'aahed',\n",
       " 'aahing',\n",
       " 'aahs',\n",
       " 'aal',\n",
       " 'aalii',\n",
       " 'aaliis',\n",
       " 'aals',\n",
       " 'aam',\n",
       " 'aani',\n",
       " 'aardvark',\n",
       " 'aardvarks',\n",
       " 'aardwolf',\n",
       " 'aardwolves',\n",
       " 'aargh',\n",
       " 'aaron',\n",
       " 'aaronic']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = load_words_from_json(\"words_dictionary.json\")[:20]\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Set\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\") # load once globally\n",
    "\n",
    "def preprocess_words(words: List[str]) -> List[str]:\n",
    "    \"\"\"Clean and lemmatize words using spaCy. \n",
    "    Removes stopwords and non-alphabetic tokens.\n",
    "\n",
    "    Args:\n",
    "        words (List[str]): Raw word list.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: Cleaned and lemmatized word list.\n",
    "    \"\"\"\n",
    "    cleaned: Set[str] = set()\n",
    "    for word in words:\n",
    "        doc = nlp(word.strip().lower())\n",
    "        token = doc[0]\n",
    "        if token.is_alpha and not token.is_stop:\n",
    "            lemma = token.lemma_\n",
    "            cleaned.add(lemma)\n",
    "    return list(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aa',\n",
       " 'aaa',\n",
       " 'aahed',\n",
       " 'aah',\n",
       " 'aal',\n",
       " 'aalii',\n",
       " 'aam',\n",
       " 'aaron',\n",
       " 'aahs',\n",
       " 'aani',\n",
       " 'aardwolf',\n",
       " 'aardvark',\n",
       " 'aargh',\n",
       " 'aaliis',\n",
       " 'aaronic']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_words = preprocess_words(words)\n",
    "cleaned_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed words with FastText (Gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "def load_word2vec_model(model_name: str = \"word2vec-google-news-300\") -> KeyedVectors:\n",
    "    \"\"\"\n",
    "    Load a pretrained Word2Vec model using Gensim.\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Gensim model name.\n",
    "\n",
    "    Returns:\n",
    "        KeyedVectors: Loaded Word2Vec model.\n",
    "    \"\"\"\n",
    "    return api.load(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "def get_vectors(words: List[str], model: KeyedVectors) -> Tuple[List[str], np.ndarray]:\n",
    "    \"\"\"\n",
    "    Get vectors for words using a pretrained model.\n",
    "\n",
    "    Args:\n",
    "        words (List[str]): Words to embed.\n",
    "        model (KeyedVectors): Loaded embedding model.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[str], np.ndarray]: (Words with vectors, Corresponding vectors)\n",
    "    \"\"\"\n",
    "    valid_words = [w for w in words if w in model]\n",
    "    vectors = np.array([model[w] for w in valid_words])\n",
    "    return valid_words, vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster with HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "\n",
    "def cluster_words(vectors: np.ndarray, min_cluster_size: int = 30) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Cluster word vectors using HDBSCAN.\n",
    "\n",
    "    Args:\n",
    "        vectors (np.ndarray): Word vectors.\n",
    "        min_cluster_size (int): Minimum size of clusters.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Cluster labels (-1 = outlier).\n",
    "    \"\"\"\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size)\n",
    "    return clusterer.fit_predict(vectors)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-label clusters with WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/thtkha/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "from nltk.corpus import wordnet as wn\n",
    "from typing import Dict\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "def get_lexname(word: str) -> str:\n",
    "    \"\"\"\n",
    "    Get WordNet semantic field for a word.\n",
    "\n",
    "    Args:\n",
    "        word (str): Input word.\n",
    "\n",
    "    Returns:\n",
    "        str: WordNet lexname or 'unknown'.\n",
    "    \"\"\"\n",
    "    synsets = wn.synsets(word)\n",
    "    return synsets[0].lexname() if synsets else \"unknown\"\n",
    "\n",
    "def label_clusters(words: List[str], labels: np.ndarray) -> Tuple[Dict[int, str], Dict[int, List[str]]]:\n",
    "    \"\"\"\n",
    "    Assign semantic labels to clusters.\n",
    "\n",
    "    Args:\n",
    "        words (List[str]): Clustered words.\n",
    "        labels (np.ndarray): Cluster labels.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict[int, str], Dict[int, List[str]]]: Cluster label names, cluster contents.\n",
    "    \"\"\"\n",
    "    cluster_dict = defaultdict(list)\n",
    "    for word, label in zip(words, labels):\n",
    "        if label != -1:\n",
    "            cluster_dict[label].append(word)\n",
    "\n",
    "    cluster_labels = {}\n",
    "    for label, cluster_words in cluster_dict.items():\n",
    "        lexnames = [get_lexname(w) for w in cluster_words]\n",
    "        top_label = Counter(lexnames).most_common(1)[0][0]\n",
    "        cluster_labels[label] = top_label\n",
    "    return cluster_labels, cluster_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def visualize_clusters(words: List[str], vectors: np.ndarray, labels: np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    Visualize word clusters using t-SNE.\n",
    "\n",
    "    Args:\n",
    "        words (List[str]): Words.\n",
    "        vectors (np.ndarray): Word vectors.\n",
    "        labels (np.ndarray): Cluster labels.\n",
    "    \"\"\"\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    reduced = tsne.fit_transform(vectors)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for label in set(labels):\n",
    "        idxs = [i for i, l in enumerate(labels) if l == label]\n",
    "        x = [reduced[i][0] for i in idxs]\n",
    "        y = [reduced[i][1] for i in idxs]\n",
    "        label_name = f\"Cluster {label}\" if label != -1 else \"Outliers\"\n",
    "        plt.scatter(x, y, alpha=0.6, label=label_name)\n",
    "    plt.legend()\n",
    "    plt.title(\"t-SNE Visualization of Word Clusters\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m words = load_words_from_json(\u001b[33m\"\u001b[39m\u001b[33mwords_dictionary.json\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m cleaned_words = \u001b[43mpreprocess_words\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m model = load_word2vec_model()\n\u001b[32m      7\u001b[39m valid_words, vectors = get_vectors(cleaned_words, model)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mpreprocess_words\u001b[39m\u001b[34m(words)\u001b[39m\n\u001b[32m     17\u001b[39m cleaned: Set[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28mset\u001b[39m()\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words:\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     doc = \u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     token = doc[\u001b[32m0\u001b[39m]\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m token.is_alpha \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m token.is_stop:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/english-learning-app/venv/lib/python3.11/site-packages/spacy/language.py:1052\u001b[39m, in \u001b[36mLanguage.__call__\u001b[39m\u001b[34m(self, text, disable, component_cfg)\u001b[39m\n\u001b[32m   1050\u001b[39m     error_handler = proc.get_error_handler()\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1052\u001b[39m     doc = \u001b[43mproc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcomponent_cfg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m   1053\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1054\u001b[39m     \u001b[38;5;66;03m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[32m   1055\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors.E109.format(name=name)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/english-learning-app/venv/lib/python3.11/site-packages/spacy/pipeline/trainable_pipe.pyx:53\u001b[39m, in \u001b[36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/english-learning-app/venv/lib/python3.11/site-packages/spacy/pipeline/transition_parser.pyx:343\u001b[39m, in \u001b[36mspacy.pipeline.transition_parser.Parser.set_annotations\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/english-learning-app/venv/lib/python3.11/site-packages/spacy/pipeline/_parser_internals/ner.pyx:275\u001b[39m, in \u001b[36mspacy.pipeline._parser_internals.ner.BiluoPushDown.set_annotations\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/english-learning-app/venv/lib/python3.11/site-packages/spacy/tokens/doc.pyx:814\u001b[39m, in \u001b[36mspacy.tokens.doc.Doc.set_ents\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/english-learning-app/venv/lib/python3.11/site-packages/spacy/tokens/doc.pyx:127\u001b[39m, in \u001b[36mspacy.tokens.doc.SetEntsDefault.values\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.11/enum.py:806\u001b[39m, in \u001b[36mEnumType.__members__\u001b[39m\u001b[34m(cls)\u001b[39m\n\u001b[32m    801\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    802\u001b[39m \u001b[33;03m    Return the number of members (no aliases)\u001b[39;00m\n\u001b[32m    803\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    804\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mcls\u001b[39m._member_names_)\n\u001b[32m--> \u001b[39m\u001b[32m806\u001b[39m \u001b[38;5;129m@bltns\u001b[39m.property\n\u001b[32m    807\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__members__\u001b[39m(\u001b[38;5;28mcls\u001b[39m):\n\u001b[32m    808\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    809\u001b[39m \u001b[33;03m    Returns a mapping of member name->value.\u001b[39;00m\n\u001b[32m    810\u001b[39m \n\u001b[32m    811\u001b[39m \u001b[33;03m    This mapping lists all enum members, including aliases. Note that this\u001b[39;00m\n\u001b[32m    812\u001b[39m \u001b[33;03m    is a read-only view of the internal mapping.\u001b[39;00m\n\u001b[32m    813\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    814\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m MappingProxyType(\u001b[38;5;28mcls\u001b[39m._member_map_)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "words = load_words_from_json(\"words_dictionary.json\")\n",
    "\n",
    "cleaned_words = preprocess_words(words)\n",
    "\n",
    "model = load_word2vec_model()\n",
    "\n",
    "valid_words, vectors = get_vectors(cleaned_words, model)\n",
    "\n",
    "labels = cluster_words(vectors, min_cluster_size=30)\n",
    "\n",
    "cluster_labels, cluster_dict = label_clusters(valid_words, labels)\n",
    "\n",
    "for cid, label in cluster_labels.items():\n",
    "    print(f\"\\n[Cluster {cid}: {label}]\")\n",
    "    print(\", \".join(cluster_dict[cid][:10]))\n",
    "\n",
    "visualize_clusters(valid_words, vectors, labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
