{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import json\n",
    "from typing import List, Tuple, Dict, Set\n",
    "import numpy as np\n",
    "import spacy\n",
    "from gensim.models import KeyedVectors\n",
    "import hdbscan\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Download NLTK WordNet data\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # Load once globally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_words_from_json(filepath: str) -> List[str]:\n",
    "    \"\"\"Load words from a JSON file where keys are words.\"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "    return list(data.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_words(words: List[str]) -> List[str]:\n",
    "    \"\"\"Clean and lemmatize words using spaCy.\"\"\"\n",
    "    cleaned: Set[str] = set()\n",
    "    for word in words:\n",
    "        doc = nlp(word.strip().lower())\n",
    "        token = doc[0]\n",
    "        if token.is_alpha and not token.is_stop:\n",
    "            lemma = token.lemma_\n",
    "            cleaned.add(lemma)\n",
    "    return list(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embed words with FastText (Gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_word2vec_model(model_name: str = \"word2vec-google-news-300\") -> KeyedVectors:\n",
    "    \"\"\"Load a pretrained Word2Vec model using Gensim.\"\"\"\n",
    "    import gensim.downloader as api\n",
    "    return api.load(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dimensionality(vectors: np.ndarray, n_components: int = 50) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Reduce the dimensionality of word vectors using PCA.\n",
    "\n",
    "    Args:\n",
    "        vectors (np.ndarray): Original high-dimensional word vectors.\n",
    "        n_components (int): Number of dimensions to reduce to.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Reduced-dimensionality vectors.\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=n_components, random_state=42)\n",
    "    reduced_vectors = pca.fit_transform(vectors)\n",
    "    return reduced_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_words_by_similarity(model: KeyedVectors, seed_words: List[str], top_n: int = 60) -> List[str]:\n",
    "    \"\"\"Filter words by finding the most similar words to the seed words.\"\"\"\n",
    "    similar_words = set()\n",
    "    for seed in seed_words:\n",
    "        if seed in model:\n",
    "            similar = model.most_similar(seed, topn=top_n // len(seed_words))\n",
    "            similar_words.update([word for word, _ in similar])\n",
    "    return list(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(words: List[str], model: KeyedVectors) -> Tuple[List[str], np.ndarray]:\n",
    "    \"\"\"Get vectors for words using a pretrained model.\"\"\"\n",
    "    valid_words = [w for w in words if w in model]\n",
    "    vectors = np.array([model[w] for w in valid_words])\n",
    "    return valid_words, vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster with HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_words(vectors: np.ndarray, min_cluster_size: int = 30) -> np.ndarray:\n",
    "    \"\"\"Cluster word vectors using HDBSCAN.\"\"\"\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster_size)\n",
    "    return clusterer.fit_predict(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-label clusters using centroid-nearest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_clusters_centroid(words: List[str], vectors: np.ndarray, labels: np.ndarray, model: KeyedVectors) -> Dict[int, str]:\n",
    "    \"\"\"\n",
    "    Assign labels to clusters using the centroid-nearest word approach.\n",
    "\n",
    "    Args:\n",
    "        words (List[str]): List of words.\n",
    "        vectors (np.ndarray): Word vectors.\n",
    "        labels (np.ndarray): Cluster labels.\n",
    "        model (KeyedVectors): Pre-trained word embedding model.\n",
    "\n",
    "    Returns:\n",
    "        Dict[int, str]: Mapping of cluster IDs to their labels.\n",
    "    \"\"\"\n",
    "    cluster_centroids = {}\n",
    "    cluster_words = defaultdict(list)\n",
    "\n",
    "    # Group words by cluster\n",
    "    for word, vector, label in zip(words, vectors, labels):\n",
    "        if label != -1:  # Ignore outliers\n",
    "            cluster_words[label].append(vector)\n",
    "\n",
    "    # Compute centroids and find nearest word\n",
    "    for label, cluster_vectors in cluster_words.items():\n",
    "        centroid = np.mean(cluster_vectors, axis=0)\n",
    "        cluster_centroids[label] = centroid\n",
    "\n",
    "    cluster_labels = {}\n",
    "    for label, centroid in cluster_centroids.items():\n",
    "        nearest_word = model.similar_by_vector(centroid, topn=1)[0][0]\n",
    "        cluster_labels[label] = nearest_word\n",
    "\n",
    "    return cluster_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_clusters(words: List[str], vectors: np.ndarray, labels: np.ndarray) -> None:\n",
    "    \"\"\"Visualize word clusters using t-SNE.\"\"\"\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    reduced = tsne.fit_transform(vectors)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for label in set(labels):\n",
    "        idxs = [i for i, l in enumerate(labels) if l == label]\n",
    "        x = [reduced[i][0] for i in idxs]\n",
    "        y = [reduced[i][1] for i in idxs]\n",
    "        label_name = f\"Cluster {label}\" if label != -1 else \"Outliers\"\n",
    "        plt.scatter(x, y, alpha=0.6, label=label_name)\n",
    "    plt.legend()\n",
    "    plt.title(\"t-SNE Visualization of Word Clusters\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main worflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load words from JSON\n",
    "words = load_words_from_json(\"words_dictionary.json\")\n",
    "\n",
    "# Preprocess words\n",
    "cleaned_words = preprocess_words(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Word2Vec model\n",
    "model = load_word2vec_model()\n",
    "\n",
    "# Define seed words\n",
    "seed_words = [\"language\", \"learning\", \"education\"]\n",
    "\n",
    "# Filter words\n",
    "filtered_words = filter_words_by_similarity(model, seed_words, top_n=60)\n",
    "print(\"Filtered Words:\", filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get vectors for filtered words\n",
    "valid_words, vectors = get_vectors(filtered_words, model)\n",
    "\n",
    "# Reduce dimensionality with PCA\n",
    "reduced_vectors = reduce_dimensionality(vectors, n_components=50)\n",
    "print(f\"Reduced Vectors Shape: {reduced_vectors.shape}\")\n",
    "\n",
    "# Cluster words using reduced vectors\n",
    "labels = cluster_words(reduced_vectors, min_cluster_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label clusters using centroid-nearest word\n",
    "cluster_labels = label_clusters_centroid(valid_words, reduced_vectors, labels, model)\n",
    "\n",
    "# Print cluster labels\n",
    "for cid, label in cluster_labels.items():\n",
    "    print(f\"Cluster {cid}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clusters\n",
    "visualize_clusters(valid_words, vectors, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
